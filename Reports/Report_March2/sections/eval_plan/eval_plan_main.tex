% What do we want to evaluate
% Why do we want to evaluate that?
For this project, the team is interested in focusing on two areas for
evaluation: the value of matching and the usability of the system.
The value of a match could be evaluated in any number of ways.  In a
perfect world, there would be a good algorithmic way to evaluate
matches as low or high quality and then a multi-year study of peer
tutoring would be done where actual students would give feedback on
the matches they recieve.  Unfortunately, the project's roughly one
month timeline makes that kind of evaluation, however valuable,
impossible.  For that reason, Section \ref{sec:evaluating-matching}
will focus on algorithmic evaluation almost exclusively.

Usability for this task is also a major concern.  One of the hilights
of the original app is its ease of use.  It is exceedingly easy to
register and schedule your first tutoring session in a matter of
seconds, not minutes, and that is something we are adamant about not
changing, and our goal is to make that even faster with repeated
sessions.  To that end, Section \ref{sec:evaluating-usability} details
a strategy for evaluating the changes being proposed in light of the
time it takes to schedule tutoring sessions before and after the enhancement.


\subsection{Evaluating Matching}
\label{sec:evaluating-matching}
% How?
The first priority for this enhancement is validating the matches
created.  Without the ability to run a longitudinal study evaluating
actual students, the only option is to generate matches on test data
and evaluate the usefulness of the matches themselves.

% Process?
To that end, this test will require hand labeling a set of randomized
students, tutoring sessions, and ratings as ``high quality'', ``low
quality'' and ``reasonable quality''.  Once that is done, the team
will need to run some form of automated tests on that data and
validate what percentage of the desired high quality matches are
actually generated by the suggestion algorithm.  There is actually a
reasonable path to do this in the current code base, as current
automated tests for the system rely on data being in a local database
for testing.  While this was originally considered a mistake by the
previous maintainers, this actually works as an advantage in this
situation.  A test database can be prepared with a number of students,
say 50 students and 300 tutoring sessions spread out randomly among
them.  Then data about what matches should be found and what ones
should not be found can be encoded into automated unit tests, which
can be re-run easily and often to help validate the effectiveness of
the suggestion algorithm.

% Data?
This does leave one major problem: where to get data and how to
label it.  This problem is one that must be solved by hand.  Test data
generators are readily available online to generate people and
calendar events easily enough which should be easily imported into
WolfTutor's Mongo database, but several hours of hand labeling will
need to take place for this evaluation method to produce useful
results.  As of the time of this writing, the team does not have a
better approach to this specific manual-labor task outside of possible
croudsourcing.  

\subsection{Evaluating Usability}
\label{sec:evaluating-usability}
% How
The next major priority is evaluating the usability of the addition to
the application.  In this case, the primary metric of usability is the
time it takes the schedule tutoring sessions in the system.  This is
something that can and should be tested with actual users rather than
relying on an algorithm.  

% When
To evaluate usability the team will conduct a focus group with ten to
twenty potential users from this CSC510 class.  Each respondent will
be asked to use both the old and the new system while being timed.
They will be asked to schedule some number of tutoring sessions one
after the other using test data generated during the previous phase of
evaluation.  The team does intend to offer the new and old system in
a different order each time, to help control for the fact that users
will not necessarily already know how to use the system the first
time, which may skew results.  

The team also intends to ask respondents to give feedback about the
matches they recieved when using the application.  This is obviously
not as interesting as if the actual tutoring sessions took place,
which is why it is not being solely relied on as the measure of the
effectiveness of this enhancement, but if these results contradict the
previous results, the enhancement may be flawed.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
